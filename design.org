* What can language models tell us about transphobia?

https://bit.ly/gc-english-2024

** revision notes
Structure:
- overview
- introduction: transphobia quotes from Shrier? or Cass, WPATH? Or the
  bills? 
- what can we learn by studying fear?
  - Sedgwick
  - not about unearthing transphobia but studying how it moves.
- ROGD and interest in fear
- deep look into llms, how they are trained
  - they work by approximation, normalization, in a mathematical
    process.
- relationship to Trans Studies and the desire to "pass"
- 

Add:
- [ ] start with quote on transphobia, from shrier, cass, or WPATH
  files?
- [ ] training process: vectors
- [ ] training process: hypothesis, loss, gradient
  - this is the work, making all of this intelligible to non-technical
    audiences, in a way that excites them.
- [ ] close readings of state bills. 

** overview
About my work customizing an AI model to study transphobia in
language--specifically, in current "anti-trans" legislation that is
proliferating across the United States. This work involves:
- data gathering, cleaning, and processing
- AI model fine-tuning
- future work in:
  - futher cleaning, processing
  - RAG methods

The focus on the fine-tuning process is to exploit the ways that
models learn, which is by a method of generalization, to inform the
study of transphobia. And how they might engage with conversations in
Trans Studies regarding transphobia, particularly as manifested in the
last several years in the false condition of ROGD, Rapid Onset Gender
Dysphoria.

Research question:
- Training is a kind of normalization of language meaning. It
  approximates. How can this be related to Trans Studies?
- ROGD is the idea that being trans is contagious. The condition
  itself implies that there is a fear of transness, a transphobia. It
  is /this fear/ that I am interested in.
- Is there a way to use the training process to study the appeal and
  seduction of transphobia? The way that transphobia /moves/?

** Thank you for having me here today.
These days, public discourse seems to be beyond the reach of (or even
desire for) deep, critical thinking. Right now, we're witnessing a
particular kind of discourse around feminism, "gender critical
feminism," that twists understandings of tolerance and equality to
perpatuate a patriarchial control over identity and expression.

** TODO antitrans quotes
One way that we've seen this is in the explosion of "anti-trans"
literature across the United States. For those who are unfamiliar,
this legislation limits trans peoples' access to basic healthcare,
public facilities, legal recognition, and more.

[ANTI-TRANS LEGISLATION MAP]

Here, you can see a map of where these bills are most concentrated
across the country, and a chart of how many bills are proposed and
passed over the last four years. Notice that this year, though we are
hardly in the fifth month, we’ve almost caught up to the total bills
for last year.

[SOME QUOTES? THAT SHOW REASONING BEHIND THESE BILLS]

SOME ANALYSIS ON THIS PAPER, ABOUT FEAR.

** what does knowledge do?
Humanists have unique tools for thinking through such discourses based
on fear and repression.

Here, I’m inspired by the scholarship of Eve Kosofsky Sedgwick, who is
a major and influential figure in Queer Studies. She is most famous
for books like /The Epistemology of the Closet/ (pictured on the left)and her essay, "Paranoid Reading and Reparative Reading" (pictured
right), where she seeks and analyzes queer themes and repressive
structures in authors like Henry James, Oscar Wilde, and Marcel
Proust.

Within the context of this project, I'm interested in Sedgwick's work
for two reasons. First, though she doesn't write about technology, her
means of analysis, particularly her manner of close-reading, lends
well to deconstructing computational concepts. The way she structures
her material, often thinking in terms of binaries and other highly
delineated structures, evokes (for me) the constraints of
computational forms. In /Epistemology of the Closet/, for example, she
expose what she calls the unstable binaries between heterosexual and
homosexual categories. Through close-readings of fiction, she exposes
the inherent instability of these binaries — where one term is not
symmetrical or simply subordinated to another, but rather, depends the
other for its meaning through “simultaneous subsumption and exclusion”
(10). Such binaries, she explains, are “sites that are peculiarly
densely charged with lasting potentials for powerful manipulation”
(10).

The second reason I'm interested in Sedgwick is because she offers
provocative ways of thinking through repressive discourses like those
based on fear. Throughout the trajectory of her career, her reading
develops from one that she calls "paranoid reading" into a new mode
called “reparative reading.” She defines paranoid reading as a
critical practice based on “the logic of repression” (a logic which
she traces to Foucault), that searches for hidden meaning in text with
the goal of exposing "truth". In her famous essay on this topic,
Sedgwick asserts that this practice, of unveiling or exposing truth,
in critical analysis, does not do much. Merely knowing that something
is true, revealing the presence of systematic oppression, injustice,
discrimination, for example (and here, Sedgwick is theorizing within
the context of the AIDs crisis), is not enough to “enjoin that person
to any specific train of epistemological or narrative consequences”
(123). Rather, Sedwicks seeks to

#+begin_quote
"Mov[e] from the rather fixed question Is a particular piece of
knowledge true, and how can we know? to the further questions: what
does knowledge do–the pursuit of it, the having and exposing of it"
(124, Touching Feeling)
#+end_quote

Sedgwick proposes a mode of “reparative reading,” which focuses on
connection rather than exposure, in which a reader allows herself to
be taken by surprise. What if, Sedgwick asks, we take something that
is typically seen as a negative, structuring force in queer identity,
like the feeling of shame, and examine how it unlocks creativity and
productivity? Sedgwick describes shame as a contagious affect, which
may be read as a mobilizing and creative force in text: she explains
that,

[SLIDE 6: SHAME QUOTES]

#+begin_quote
“Shame—living, as it does, on and in the muscles and capillaries of
the face—seems to be uniquely contagious from one person to another."
(63 Touching Feeling).
#+end_quote

She also describes shame as:

#+begin_quote
“not a discrete intrapsychic structure, but a kind of free radical
that (in different people and different cultures) attaches to and
permanently intensifies or alters the meaning of—of almost anything: a
zone of the body, a sensory system, a prohibited or indeed a permitted
behavior, another affect such as anger or arousal, a named identity, a
script for interpreting other people’s behavior toward oneself” (62)
#+end_quote

She demonstrates this reading practice by analyzing metaphors that are
made possible through shame, for example in the fiction of Henry
James. In one example, she connects moments of "blushing" and
"flushing" to a fantasy of the skin being entered. Shame, in this
reading, is a way of pulling other affects and images into relation.
This is opposed to paranoid reading, which might plumb shame for what
it reveals about a hidden or repressed sexuality. She explains that,
“When we tune into James’s language on these frequencies, it is not as
superior, privileged eavesdroppers on a sexual narrative hidden from
himself; rather, it is as an audience offered the privilege of sharing
his exhibitionistic enjoyment and performance of a sexuality organized
around shame” (54).
- shame as something that spreads, opens, makes new kinds of
  connections possible; rather than something to simply prove the
  existance. What does /shame do/?

** fear
I'm interested in this move that Sedgwick makes, of taking what is
typically seen as a negative, repressive affect, like shame, and
seeing how it opens up possibilities for reading new connections in
text. Specifically, I wonder one might read something productive in
fear--of the phobias--that pervade anti-trans discourses. 

In my current work, I am exploring this fear in anti-trans legislation.

[ROGD PAPER]

For example, one of the things I'm studying is the (now disproved)
clinical phenomenon of so-called "Rapid Onset Gender Dysphoria"
(ROGD), which stipulates that trans-ness can be contagious among
adolescents. The phrase was coined by Dr. Lisa Littman in her study of
parents of transgender youth, published in 2018. In her study, Littman
interviews over 250 parents of transgender children and concludes that
what she calls "peer contagion" of gender dysphoria may be a
contributing factor for adolescents who decide to transition. Almost
immediately following its publication, Littman's study was criticized,
including by the publisher, and its methodology and findings have been
disavowed by every major medical association since then.

Although ROGD is not recognized as a valid diagnosis, it has been used
and is still used as fodder for anti-trans propaganda and
discrimination. It has made its way into the public lexicon, appearing
in books, shows, and most importantly for my project, legislative
bills that are being written, debated, and passed across the United
States.

ADD SHRIER: SOME QUOTES ABOUT RODG

I’m interested in this threat of gender transgression, and
specifically, in the language outlawing gender transgression, of
transitioning from one gender to another, or of opting out of binary
systems of gender. Why is this particular kind of transgression so
controversial among a large part of our population? Why is the fear of
this transgression itself so contagious?

** processing and training
To study this transphobia, I've decided to train an LLM off
definitions of gender (and related terms) from the anti-trans bills. I
am interested in how these models are created, how they are "trained,"
so to speak, so I can trace how they perpetuate biases, like
transphobia, from their training data into the text that they
generate.

In what follows, I'm going to outline a bit of the data gatherering,
processing, and model training that I've been doing for a little over
the last year, that I've worked on this project.

[[./img/datasets_hf.png][image of dataset from HF]]

The first dataset that I created, which is now available on
HuggingFace Datasets (for those of you who don't know, a platform for
sharing Machine Learning projects and tools, much like Github),
consists of definitions of "gender" and related terms from
congressional and senate bills, from the last two years. It consists
of 82 rows, a very small dataset, which I gathered and cleaned using
Python programming.

[[./img/df.png][image of df of bills]]

Here's an image of the bills that I gathered and scraped from
congressional servers. Here, I had to download a dataset of the bill
ID numbers from the congress.gov website. Then I wrote a web scraper
to get the plain text of all the bills by their ID. After gathering
the bills, I went through an intensive data preparation process, which
involved cleaning the text and extracting definitions of gender and
related terms from it. I'll highlight some of the major moves from
this process.

First, for those of you familiar with Natural Language Processing
methods, I created an Named Entity Recognizer to recognize terms
related to gender and related terms. 

[[./img/ner.png][image of ner code]]

You can see here a list of labels, organized into the general
categories "sex", "gender", and "sexulaity", with each label
specifying a pattern, like the phrase "biological sex" for example.
I tried to include various formulations of each term, for example,
"transgender" is delineated three ways, as a single word, as a
two-word phrase, and as a hypthenated word. This ensures that I would
capture all or most instances of the terms.

Then, I used that entity recognizer as a basis for a pattern matcher,
which would search for those phrases if they are contained within a
definition. 

[[./img/matcher.png][image of matcher code]]

Here you can see the pattern matcher's logic. It starts by searching
for punctuation (specifically, I'm looking for a quotation mark, which
typically surrounds definitions), then looking for a gender term (from
the recognizer), then some wild card terms, just in case there are
extra words or punctuation in the definition, and finally, some terms
that are common in definitions, like "means", "signifies", or
"includes." For those of you who can read some Python, you'll
recognize that I've written comments (indicated by the hashtag
character) that describes what parts of the matcher is doing.

[[./img/matcher_results.png][image of the matcher results]]

Then, I ran the matcher to extract the definitions from the bills.
Here are some of the initial results from that extraction. You can see
that the matcher was sensitive enough to capture longer phrases, like
"gender transition surgery means" as well as variants of how
definitions are constructed, using the word "includes" instead of
"means", for example.

After extracting the definitions, I then cleaned them up and formatted
them into a neat (or neater) list of definitions. For that, I used
regex (Regular Expressions). The final output then contains
definitions like the following:

#+begin_quote
'The term gender identity means a persons self-perception of their gender or claimed gender, regardless of the persons biological sex.',
'The term gender means the psychological, behavioral, social, and cultural aspects of being male or female.',
 'The term gender transition means the process in which an individual goes from identifying with and living as a gender that corresponds to his or her biological sex to identifying with and living as a gender different from his or her biological sex, and may involve social, legal, or physical changes.',
 'The term biological sex means the indication of male or female sex by reproductive potential or capacity, sex chromosomes, naturally occurring sex hormones, gonads, or internal or external genitalia present at birth.',
#+end_quote

Right now, I am interested how these assumptions are being constructed
in subtle ways, in seemingly harmless formulations. For example, in
the first definition, I am interested in the words "self-perception"
and "claimed", and how a view of gender identity as a subjective
experience engages with behavioral dimensions of gender expression, at
least as it has been theorized by scholars like Judith Butler. I am
also interested in the word “regardless,” which appears often, in
about half of the definitions, and suggests a kind of contrast between
sex and gender that seems to reify some kind of binary opposition or
tension between the two. In other words, gender as being defined
without regard to sex, as if notions of gender and sex do not
influence each other, and never blend into one another, or make
productive use of each other. Again I'm thinking here of Judith
Butler, and her famous (and contentious) claim that even biological
sex is a discursive phenomenon.

As I continue to build and clean my dataset (which is a long,
technical process that involves data gathering, processing, and
cleaning), I've also been dabbling with using them to train AI models.

As all of you probably know, the training process begins with the
model scraping and processing massive amounts of text from the
internet. From this training data, it develops an understanding of
what words mean based on context, compile numerical probabilities for
each word relationship to other words in the database. It represents
these probabilities with numbers, with actually a very large list of
numbers, known technically as "word vectors."

[KING - MAN + WOMAN = QUEEN]

Here is a famous formula that introduced this technology “word
vectors” to the world, which comes from the paper “Efficient
Estimation of Word Representations in Vector Space,” which was
published by Google researchers in 2013.

[WORD VEC PAPER]

In this formula, the idea is that by taking all the numbers that
represent king, then subtracting the ones that represent man, and
adding the ones that represent woman, you will get queen. I won't get
into the sexism of this formula (what exactly is being subtracted, for
example? is it a biological thing, a social thing?), but I want to
point out that it has great currency as it is the formula that
introduced this technology to the world.

You can think of these numbers, or scores, functioning like
definitions, which represent the word's meaning for the computer.
Here's an example of the vector for the word "woman." (taken from a
famous word vector dataset called "GLOVE" based off of Twitter data).

[WOMAN VECTOR]

To us, these scores look just like a long list of numbers, but to a
computer, the scores represent a given word's meaning through its
relationship to every other word in the entire dataset of words.
That's why, by the way, these models are so large, and why they take
so long to train. It's because every single word is represented by a
massive list of probabilities, probabilities for how that word relates
to every other word in the language. A language model will generate
content by doing math with the scores attached to each word in its
database. And the math that they use to make generate text is actually
math that many of us have heard of before in math class: things like
matrix multiplication and cosine similarity.

Prediction, in other words, pervades the whole process. And
prediction opens a connection between Machine Learning and Trans
Studies concepts. To demonstrate this connection, I'm going to go into
a bit of detail behind the training process for these tools,
explaining some of the mathematical operations in a way that is
(hopefully) intelligible to non-experts.

So what are these operations, and how do they work? When a text
generation model is being trained, it is given a prompt word, let's
say "woman", and it guesses which word ought to follow "woman." After
making some guess, let's say it guesses "flies," it then compares it's
prediction with the actual word from the training sample, "runs" or
"sings," for example. Once it sees the actual, correct answer, then it
modifies its data (represented by the word vector), for "woman." With
enough examples, the model can then create a robust enough vector for
woman so that it can use this term appropriately even within different
contexts.

To create these word vectors, there are three steps, each representing
an important mathematical function.

[SLIDE LIST OF FUNCTIONS]

1. first, the hypothesis function
2. second, the loss function
3. third, the minimizing loss function

The hypothesis function starts the process. Because the machine
doesn't know what words mean, it has to "guess." So it populates each
word with a vector, consisting of random numbers. It's a starting
point.

After making this guess, it moves to the loss funciton. Here, the
machine will check its prediction against the actual result. It's
prediction will be wrong, maybe even wildly wrong, but that doesn't
matter. It compares between the two, the prediction and the result,
and calculates the difference between them. This calculation is known
as the "loss."

Finally, it moves to the minimizing loss function, which employs
algorithms from calculus (like gradient descent) in order to /very
slightly/ minimize the loss. In other words, it adjusts the original
prediction so that it is slightly closer to the intended result. The
adjustments here are very small, incremental. Because it doesn't know
the correct answer, it makes a huge number of guesses. This may seem
inefficient, but with enough guesses, it can actually adjust the
numbers until there is /no difference/ or /almost zero difference/
between our prediction and the actual result.

** TODO approximation --> passing

They work by approximation. A kind of normalization of language. Each
step of the process it inches toward this goal

They are turning semantic expressivity into something that can be
computed and predicted. There's a kind of grounding here.

There is a connection between how language models approach language,
what they do to language (the normalization, approximation) of
language, and what Trans Studies scholars defines as a central desire
to pass.



This method of prediction is a way of normalizing, approximating, how
language works by using math.


So, put simply: it generalizes how language works by studying examples
of language forms. Given how much the training data, and the specific
configurations of words in the training data, affect the model's text
output, I am very interested in using AI tools to study anti-trans
bias, and particularly, the fear of contagion, of ROGD.


** plausibility 
Leaving aside all the hype about AI, and whether or not it is
“intelligent,” or moving toward what the industry calls “general
intelligence,” AI tools like large language models are really good at
one thing: at making predictions. At generating content that is
plausible. This is a fascinating phenomenon, because it makes them
very good at guessing or improvising, but not at all good at being
creative, at innovating. A language model can only generate what it
has already seen before. Even a phenomenon like “hallucination,” that
a language model spews text that has no bearing in reality, is based
on the tendency of models to repeat what they've already seen. They
hallucinate not because they are creative or random, but because they
are designed from statistical processes to generate what is most
plausible rather than accurate.

This tendency toward plausibility creates an interesting perspective
for me to think through how Trans Studies scholars have characterized
trans affects. Typically, these scholars describe trans affective
modes by distinguishing them from "queer" modes. In a roundtable
called "Thinking with Trans Now" published in Social Text, trans
studies scholar Eliza Steinbock explains,

“trans analytics have (historically, though not universally) a
different set of primary affects than queer theory. Both typically
take pain as a reference point, but then their affective interest
zags. Queer relishes the joy of subversion. Trans trades in quotidian
boredom. Queer has a celebratory tone. Trans speaks in sober detail.
Perhaps the style of trans studies has been for the most part realist,
but this should not be mistaken for base materialism. Even speculative
thinking requires enough detail to launch into new realms.”

Other trans scholars like Marquis Bey and Andrea Long Chu have made
similar points; with Bey making the point that queer's intervention
can be described as "anti" or militant, while trans is "non" or based
in refusal ("Thinking with Trans Now"); and Chu has remarked that
trans studies, rather than resisting norms, "requires that we
understand–as we never have before–what it means to be attached to a
norm, by desire, by habit, by survival" ("After Trans Studies" 108).

This makes me wonder, could AI-generated text, as a kind of
approximation, a normalization, of its training data, be used to study
the attachments to norms and the quotidian that characterizes trans
affective modes? Could the same processes also be used to study the
attachment to norms that characterizes the opposite movement, in
transphobia, like perspectives driven by the fear of ROGD? What might
outputs from AI text generation suggest about the allure, the threat,
the “seduction,” as Trans Studies scholar Cassius Adair puts it, of
gender transgression?

While this project might sound very ambitious, I'll admit that, so
far, my results are not very encouraging. I need to continue to add
more training data and to tweak my model configuration, probably
numerous times, before I find something really interesting.

Nonetheless, here are some excerpts of my language generated by my
model, which I trained by feeding it some examples of anti-trans
legislation that I have already prepared.

[SLIDE SHOWING THE GENERATED TEXT]

Here, you can see the prompt text (so, text that I entered as a prompt
to the model, in italics) and the AI model’s responses (where it
provides a continuation of my prompt) in normal, unitalicized text.

As you can see from skimming the results, the models are showcasing
the tendency toward plausibility, specifically in the tendency to
repeat itself, which is a fascinating concept in machine learning.

I will close now by coming back to this idea of fear, and particularly
the fear of contagion, which drives some strains of transphobia.
Cassius Adair offers a useful perspective for thinking through the
fear of contagion. In his study of trans erotics, and specifically
“trans for trans” or "t4t erotics," Adair asks, "Why shouldn't
transness be transmissible or contagious? Why can't the erotic be a
site of producing trans identity or practices?" He points out that,
after all, cis people do it all the time: they use sexuality and
sexual encounters as sites of identity formation.

Here, I see Adair doing for contagion what Sedgwick does for shame:
turning something that is traditionally seen as a negative into
something that may be generative and productive.

It is the same kind of thing I hope to accomplish with this project,
and something that I think is possible by using the tools that we gain
in English departments—that is—by close reading, or what Sedgwick
calls, "imaginative close reading."

This is a kind of reading that allows one to take what has been a tool
of oppression and turn it into a creative resource. Sedgwick explains
that this kind of reading exposes “the ways selves and communities
succeed in extracting sustenance from the objects of a culture—even of
a culture whose avowed desire has often been not to sustain them”
(Touching Feeling 151). Thank you.

** pattern matching code

lower: gender

is_punct: true

lower: means, signifies, is defined


