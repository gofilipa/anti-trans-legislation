* notes
** round 1
- [X] revise abstract
  - to two myths
  - emphasize binary thinking as way of understanding myths.
- [X] revise QS overview paragraph to explain why QS is my choice:
  - gender performativity as a de facto paradigm for understanding
    social construction of gender
  - division of sex/gender affects current understanding of trans and
    intersectional feminism. 
- [X] revise motivation -> problematizing assumptions
  - humanism problematizes assumptions
- [X] revise urgency -> binary as thought
  - gap in research considering the binary model
  - one effect: "equity not the same as equality"
- [X] revise lit reviews
  - existing schemas of bias -> my conceptualization of bias
  - queer studies on binaries
    - adding Amin on heterosexual/homosexual
- [X] cut myth 1
- [X] revise myth 2/1
- [X] revise myth 3/2
  - categorical assumption not only obscures particular type of bias,
    but also suggests bias can be excised
    - example with debias, creating equality sets that can be
      levelled.
    - neglects embeddedness of word vectors
- [X] add discussion
  - powerful manipulation of binaries (Halberstam). 
- [X] revise conclusion

** round 2
- [X] shift language to binary thinking (from binary forms)
- [X] add to QS section: QS is unique for the way it conceptualizes
  binaries: not only dissolving the essentialism of the gender binary,
  but binary thinking that structures power relations.
- [X] move urgency (no research on binaries in NLP) above motivation
  (humananism to examine assumptions).

- [X] add halberstam to on binaries section
- [X] reorganize last paragraph on binaries section

- [X] myth 1: foreground binary thinking
- [X] myth 1: streamline description of IAT and its applicaiton to
  WEAT
- [X] myth 1: add point about conscious/unconscious (see notebook)

- [X] myth 2: expand analysis of delicate and math, distinguishing
  stereotype from derogatory; compare with bachelor and spinster.
- [X] myth 2: separate out gender swapping methods.

- [X] discussion: expand summary (from notebook)
- [X] discussion: remove examples that aren't directly relevant
- [X] discussion: add halberstam: identity crossing multiple
  threshholds, gender geometries.

- [X] conclusion: summarize binary thinking
** round 3
Conform to [[https://aclrollingreview.org/authorchecklist][submission checklist]], including: 
- [ ] convert to tex
- [ ] add bias statement section
- [ ] add limitations section
- [ ] anonymize submission

When submitting: 
- [ ] nominate self to review
- [X] add semantic scholar to OR profile
** to read:
- Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, Kai-Wei Chang, and
  Nanyun Peng. 2023. “Kelly is a Warm Person, Joseph is a Role Model”:
  Gender Biases in LLM-Generated Reference Letters. In Findings of the
  Association for Computational Linguistics: EMNLP 2023, pages
  3730–3748, Singapore. Association for Computational Linguistics.
  https://aclanthology.org/2023.findings-emnlp.243.pdf

  - measures bias as a result of prompting
    - prompts LLMs to write letters of recs; some prompts have
      biographical info, others have no context
  - studies bias in language according to three categories:
    - lexical content: word choices
    - language style: positivity, formality of language 
    - hallucination
  - generates male and female examples from wikibias (substituting
    original wiki names/pronouns with one male and one female) to feed
    into the prompts
  - notice that models "tend to use gender-stereotypical words", but
    opt to use WEAT to create more "interpretable results" -- to
    evaluate if words in male & female docs are associated to
    stereotypically male or female words.
  
- Daisuke Oba, Masahiro Kaneko, and Danushka Bollegala. 2024.
  In-Contextual Gender Bias Suppression for Large Language Models. In
  Findings of the Association for Computational Linguistics: EACL
  2024, pages 1722–1742, St. Julian’s, Malta. Association for
  Computational Linguistics.
  https://aclanthology.org/2024.findings-eacl.121/

  - They use prompting methods combined with Crows-Pairs and measure
    likelihoods that a model will pick a stereotypical over a
    nonstereotypical response. 
  - mitigating bias through prompting, what they call "bias
    suppression": "providing textual preambles constructed from
    manually designed templates and real-world statistics, without
    accessing to model parameters"
  - Not about bias evaluation, but mitigation.
    - But these methods, the authors admit, do not have downstream
      effects. They work best when a user does not have access to
      model's parameters.
    - IOW, this is a band-aid, one which takes responsibility away
      from the model developers.

- Tsika, Noah. “CompuQueer: Protocological Constraints, Algorithmic
  Streamlining, and the Search for Queer Methods Online.” Women’s
  Studies Quarterly, vol. 44, no. 3/4, 2016, pp. 111–30. JSTOR,
  http://www.jstor.org/stable/44474065. Accessed 12 Mar. 2025.
  
  - What we think of as queer "queer methods" actually resists the
    workings of the machine. Machinic processes only engage queerness
    after it has been stabilized and assimilated.
  - The point is to use these machinic processes to recognize our own
    exclusion.

- Thiemo Wambsganss, Xiaotian Su, Vinitra Swamy, Seyed Neshaei, Roman
  Rietsche, and Tanja Käser. 2023. Unraveling Downstream Gender Bias
  from Large Language Models: A Study on AI Educational Writing
  Assistance. In Findings of the Association for Computational
  Linguistics: EMNLP 2023, pages 10275–10288, Singapore. Association
  for Computational Linguistics.
  https://aclanthology.org/2023.findings-emnlp.689
  - Recent example of using WEAT

- Aishik Rakshit, Smriti Singh, Shuvam Keshari, Arijit Ghosh
  Chowdhury, Vinija Jain, and Aman Chadha. 2025. From Prejudice to
  Parity: A New Approach to Debiasing Large Language Model Word
  Embeddings. In Proceedings of the 31st International Conference on
  Computational Linguistics, pages 6718–6747, Abu Dhabi, UAE.
  Association for Computational Linguistics.
  https://aclanthology.org/2025.coling-main.450
  - developed Mean Average Cosine Similarity (MAC) as an extention of
    WEAT.
  
    
- Os Keyes. 2018. The Misgendering Machines: Trans/HCI Implications of
  Automatic Gender Recognition. Proc. ACM Hum.-Comput. Interact. 2,
  CSCW, Article 88 (November 2018), 22 pages.
  https://doi.org/10.1145/3274357

More recent reserach on 'pure generation': 
- https://arxiv.org/pdf/2401.15585
  - "CoT" chain of thought prompting reduces "unconscious social bias"
    in LLMs.
    - making implicit correlations between conscious and unconscious
      bias in humans to that in machines; assumes that because CoT is
      explicit that it accesses a "conscious" mode of reasoning.
      - like the IAT, attachment to the discrepancy between conscious
        and unconscious bias, taking away attention from the different
        kinds of bias.
      - "we find that CoT encourages an LLM to be aware of its hidden
        biases" [Kaneko et al. 2024]
- https://arxiv.org/pdf/2402.11190
  - uses "indirect probing" to get at "implicit bias." 
- https://aclanthology.org/2024.icnlsp-1.42
  - using prompting to generate biased datasets, then evaluated. 
  - it is fascinating, isn't it -- the ways these tools can refuse to
    supply gendered output, but we know they are biased.
  - bias hides; not in conscious/unconscious ways, but by its
    different effects. Perhaps, just perhaps, by studying bias as an
    implicit or subconscious phenomenon, we are burying it deeper and
    deeper from our view.
  - claim that prompting is more accessible for users when it comes to
    closed models. 
- https://aclanthology.org/2023.acl-short.30
  - claims that prompting is better to avoid re-training. 
  - "large-scale retraining of these models from scratch is both time
    and compute-expensive, a variety of approaches have been
    previously proposed that de-bias a pre-trained model"
- https://aclanthology.org/2024.emnlp-main.13
  - when people discuss the benefits of prompting, some of the reasons
    is that its more accessible to end users who want control over
    their specific use cases, or that the model parameters for closed
    models (like Chat GPT) are not available for customization by the
    user.
    - But, from the perspective of the model developers, what this
      does is push off the responsibility for debiasing to the end
      user. The model developers are absolved from the harms that
      their models perpetuate.
- https://arxiv.org/abs/2404.01349


** peer review notes
The paper that I wrote was about three myths about bias based on the
binary. But it's not so tight as that--the connection between binary
and "excised". 

Reviewers suggest:
- adding sources on gender and feminism:
  - engagment with "gender studies literature", like "Data Feminism"
    by Klein and D'Ignazio.
    
- BIAS IS CATEGORICAL
  - tie more recent analysis of "pure generation" in NLP to dicussion of
    WEAT
    - "feminst analyses of gender biases" (links included)
    - methods mentioned "may not represent the current state-of-art",
      evaluation is moving away from intrinsic embedding-based metrics
      or extrinsic template-based methods to purely generative models.
    - WEAT seems to dominate most of the discussion, however there are
      several other examples to illustrate how bias is categorically
      conceptualised in research.
  - binaries not always good/bad in WEAT
    - WEAT measurement involves measuring associations with concepts
      of different variety for example, measuring gender associations
      with maths vs arts , career vs family , etc. So it is not always
      (infact, rarely) good vs bad conceptualisation. Although since
      WEAT uses binary gender representations in doing so, binary
      category argument holds which can be highlighted more clearly in
      the discussion of this myth (Section 3.2). I do believe that
      WEAT is often unreliable and unstable, but I think the resulting
      score is a measure of strength of association with these
      concepts which is more interpretative/suggestive rather than
      plainly good/bad.
      
- how I'm using "queer"
  - the paper mentions queer but "does not specifically address the
    situations of LGBTQIA+ people".
    
- style
  - add more analysis around quotes, consider moving some quotes to
    analysis
  - author's voice drowned out by quotes, perspectives, to drop ideas
    and then move on, so that the argument doesn't come through and the
    reader has to string it together.

- overal argument structure
  - motivation
    - how "equity is not the same as equality"
  - urgency
    - articulate why my critiques benefit from a queer understanding.
      What does it add to the literature?
    - treating bias as categorical or even quantifiable can undermine
      the importance of measuring its actual effect on diverse users,
      and strategies that seem to level it could actually just mask
      these biases which can be manifested in other forms like
      downstream applications.
    - how "the underlying strategy of using word embeddings continues
      to influence a distinct trajectory of development"
  - originality
    - this paper presents an original perspective by conceptualizing
      these myths, summarizing critical considerations for NLP bias
      research. While these myths build on existing critiques of bias
      evaluation and mitigation, their grounding in Queer Studies makes
      them especially relevant for NLP bias researchers and
      practitioners.
  - A separate section such as Discussion to identify possible
    directions for addressing these myths
    - the author's debunking of the three myths does not directly link
      to the conclusion; an explanation of how the traditional binary
      is inadequate is not enough evidence to claim another way is
      better.
  - strengthening connection between queer studies on binaries and the
    myths.


** BANK

Bias is not neutral. 
- Devinney et al 2022: "Masculine and feminine genders are also
  generally presented as “opposites" which can be “swapped" for
  each other; placed on each end of a linear scale of bias; or
  used to define vector space directions."

- Devinny et al 2022: on "beards"
  - "There may be a statistically strong association between
    masculine nouns and beards, and beards are often a part of
    masculine performativity, but that does not make it a
    foolproof indicator of gender for an individual... The
    particular example of beard with “male nouns” is also
    ironic, as beard can in specifically refer to a woman whom
    a gay man is dating to hide his sexuality – making it a
    feminine noun in these cases... We must know... what
    discourse (the beard on someone’s face, or the beard they
    are dating?) make up the context to make a judgement about
    how meaningful a particular sentence or association is."

data contexts: gender
- calling for interdisciplinarity, specifically incorporating
  Feminist, Gender, and Queer Studies into NLP (Devinney et al
  2022). 
- theories of gender, Butler, Prosser, etc
- close readings of definitions of gender from the dataset
- small words like "regardless" reifying binaries 

data contexts: trans
- trans studies investments, trans vs queer
- transphobia based on contagion
- transphobia in bill titles? 
** bias is one dimensional
- [ ] Bias is one dimensional
  - [ ] Equalized Odds[fn:11] measures how well models perform, how
    "accurate" they are, prompted by different identities.
    - Nemani et al 2023: "measures the degree to which a model’s
      predictions are equal across different demographic groups, such
      as males and female. In the context of gender bias, the metric
      can be used to assess whether the model is making equally
      accurate predictions for male and female inputs."
  - [ ] May et al 2019: They apply WEAT to sentence level tests, enabling
    them to explore complex kinds of intersectional bias (citing hooks
    and Crenshaw) and find the method unreliable perhaps due to
    difficulty in simplifying concepts/contexts in sentence-level
    inputs.
  - [ ] FISE procedure (Charlesworth et al 2024).
    - “Interestingly, the data show less support for class-centrism,
      i.e. rich does not dominate frequencies in language to the same
      extent that White supersede Black, or men supersede women. For
      example, *Black Poor* (6% of traits) and *Black Rich* (5% of
      traits) are similar in frequency showing that the low frequency
      of traits associated with *Black* is not altered even after
      including the dominant class group *Rich*. Perhaps class may be
      less of a marked category in language: we may be unlikely to
      point out that someone is *rich*, unless it is extreme wealth,
      because categorizing class is prone to subjective judgments of
      wealth cues ([31](javascript:;)). In contrast, race and gender
      may be relatively less ambiguous in categorizations and
      therefore more likely to be noted in language and to shape trait
      frequencies.” (Charlesworth et al 2024). 
      - makes assumptions about visibility and invisibilty of certain
	identities, like class, saying that it's not as prevalent
	because being "rich" or "poor" isn't as marked in language;
	whereas class vs race visibility has been well theorized in
	humanities.

** distributional hypothesis in WEAT
As mentioned
above, WEAT takes the distributional hypothesis in linguistics, that
"the statistical contexts of words capture much of what we mean by
meaning," and apply it to computer science (Caliskan et al 2017). The
idea is that the word embeddings, which represent word meaning in
numerical form, translate the "statistical contexts" of words into the
vector space of about 300 dimensions, in the case of the Global
Vectors for Word Representation (GloVe) model. 

** word embeddings continue to influence work today:
- Despite these criticisms, the underlying strategy of using word
  embeddings continues to influence a distinct trajectory of
  development for measuring and mitigating bias. For example, both
  SEAT (The Sentence Embedding Association Test) [May et al. 2019] and
  SentenceDebias [Liang et al. 2020], expand the use of single-word
  vector representations to sentence-level representations.
** There are methods in NLP that reformulate the traditional binary to
mitigate gender bias. These methods, what Devinney et al. [2022] call
"trans-inclusive methodologies," expand the traditional binary. For
example, Hansson et al. [2021] incorporate a gender neutral pronoun
"hen" in Swedish into their Wino-gender dataset. Additionally, Dinan
et al. [2020] expand the classification of gender in their dataset to
include "neutral" and "unknown." Crowd-sourced and participatory
datasets also contribute to this effort, namely when they are done by
participants of the community, like WinoQueer [Felkner et al. 2023].
Such work take exploratory and crucial steps on the path to gender
equity in language systems.
