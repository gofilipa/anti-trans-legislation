
@inproceedings{klein:2024,
	address = {New York, NY, USA},
	series = {{FAccT} '24},
	title = {Data {Feminism} for {AI}},
	isbn = {9798400704505},
	url = {https://dl.acm.org/doi/10.1145/3630106.3658543},
	doi = {10.1145/3630106.3658543},
	abstract = {This paper presents a set of intersectional feminist principles for conducting equitable, ethical, and sustainable AI research. In Data Feminism (2020), we offered seven principles for examining and challenging unequal power in data science. Here, we present a rationale for why feminism remains deeply relevant for AI research, rearticulate the original principles of data feminism with respect to AI, and introduce two potential new principles related to environmental impact and consent. Together, these principles help to 1) account for the unequal, undemocratic, extractive, and exclusionary forces at work in AI research, development, and deployment; 2) identify and mitigate predictable harms in advance of unsafe, discriminatory, or otherwise oppressive systems being released into the world; and 3) inspire creative, joyful, and collective ways to work towards a more equitable, sustainable world in which all of us can thrive.},
	urldate = {2024-11-25},
	booktitle = {Proceedings of the 2024 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Klein, Lauren and D'Ignazio, Catherine},
	month = jun,
	year = {2024},
	pages = {100--112},
	file = {Full Text PDF:/Users/fcalado/Zotero/storage/67244GJV/Klein and D'Ignazio - 2024 - Data Feminism for AI.pdf:application/pdf},
}

@article{charlesworth:2024,
	title = {Extracting intersectional stereotypes from embeddings: {Developing} and validating the {Flexible} {Intersectional} {Stereotype} {Extraction} procedure},
	volume = {3},
	issn = {2752-6542},
	shorttitle = {Extracting intersectional stereotypes from embeddings},
	url = {https://doi.org/10.1093/pnasnexus/pgae089},
	doi = {10.1093/pnasnexus/pgae089},
	abstract = {Social group–based identities intersect. The meaning of “woman” is modulated by adding social class as in “rich woman” or “poor woman.” How does such intersectionality operate at-scale in everyday language? Which intersections dominate (are most frequent)? What qualities (positivity, competence, warmth) are ascribed to each intersection? In this study, we make it possible to address such questions by developing a stepwise procedure, Flexible Intersectional Stereotype Extraction (FISE), applied to word embeddings (GloVe; BERT) trained on billions of words of English Internet text, revealing insights into intersectional stereotypes. First, applying FISE to occupation stereotypes across intersections of gender, race, and class showed alignment with ground-truth data on occupation demographics, providing initial validation. Second, applying FISE to trait adjectives showed strong androcentrism (Men) and ethnocentrism (White) in dominating everyday English language (e.g. White + Men are associated with 59\% of traits; Black + Women with 5\%). Associated traits also revealed intersectional differences: advantaged intersectional groups, especially intersections involving Rich, had more common, positive, warm, competent, and dominant trait associates. Together, the empirical insights from FISE illustrate its utility for transparently and efficiently quantifying intersectional stereotypes in existing large text corpora, with potential to expand intersectionality research across unprecedented time and place. This project further sets up the infrastructure necessary to pursue new research on the emergent properties of intersectional identities.},
	number = {3},
	urldate = {2025-01-08},
	journal = {PNAS Nexus},
	author = {Charlesworth, Tessa E S and Ghate, Kshitish and Caliskan, Aylin and Banaji, Mahzarin R},
	month = mar,
	year = {2024},
	pages = {pgae089},
	file = {Full Text PDF:/Users/fcalado/Zotero/storage/L8U4QGQV/Charlesworth et al. - 2024 - Extracting intersectional stereotypes from embeddi.pdf:application/pdf;Snapshot:/Users/fcalado/Zotero/storage/IHKGM3QP/7626925.html:text/html},
}

@article{ainsworth:2015,
	title = {Sex redefined},
	volume = {518},
	copyright = {2015 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/518288a},
	doi = {10.1038/518288a},
	abstract = {The idea of two sexes is simplistic. Biologists now think there is a wider spectrum than that.},
	language = {en},
	number = {7539},
	urldate = {2025-01-22},
	journal = {Nature},
	author = {Ainsworth, Claire},
	month = feb,
	year = {2015},
	note = {Publisher: Nature Publishing Group},
	keywords = {Genetics, Developmental biology, Medical research, Systems biology},
	pages = {288--291},
	file = {Full Text PDF:/Users/fcalado/Zotero/storage/35789US7/Ainsworth - 2015 - Sex redefined.pdf:application/pdf;Snapshot:/Users/fcalado/Zotero/storage/S9KMM7ED/518288a.html:text/html},
}

@article{hyde:2019,
	title = {The future of sex and gender in psychology: {Five} challenges to the gender binary},
	volume = {74},
	issn = {1935-990X},
	shorttitle = {The future of sex and gender in psychology},
	doi = {10.1037/amp0000307},
	abstract = {The view that humans comprise only two types of beings, women and men, a framework that is sometimes referred to as the “gender binary,” played a profound role in shaping the history of psychological science. In recent years, serious challenges to the gender binary have arisen from both academic research and social activism. This review describes 5 sets of empirical findings, spanning multiple disciplines, that fundamentally undermine the gender binary. These sources of evidence include neuroscience findings that refute sexual dimorphism of the human brain; behavioral neuroendocrinology findings that challenge the notion of genetically fixed, nonoverlapping, sexually dimorphic hormonal systems; psychological findings that highlight the similarities between men and women; psychological research on transgender and nonbinary individuals’ identities and experiences; and developmental research suggesting that the tendency to view gender/sex as a meaningful, binary category is culturally determined and malleable. Costs associated with reliance on the gender binary and recommendations for future research, as well as clinical practice, are outlined. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
	number = {2},
	journal = {American Psychologist},
	author = {Hyde, Janet Shibley and Bigler, Rebecca S. and Joel, Daphna and Tate, Charlotte Chucky and van Anders, Sari M.},
	year = {2019},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Psychology, Future, Human Sex Differences, Neuroendocrinology, Neurosciences},
	pages = {171--193},
	file = {Snapshot:/Users/fcalado/Zotero/storage/YTYY4BP4/doiLanding.html:text/html},
}

@article{joel:2021,
	title = {Beyond the binary: {Rethinking} sex and the brain},
	volume = {122},
	issn = {0149-7634},
	shorttitle = {Beyond the binary},
	url = {https://www.sciencedirect.com/science/article/pii/S0149763420306540},
	doi = {10.1016/j.neubiorev.2020.11.018},
	abstract = {The paper reviews the relations between sex and brain in light of the binary conceptualization of these relations and the challenges posed to it by the ‘mosaic’ hypothesis. Recent formulations of the binary framework range from arguing that the typical male brain is different from the typical female brain to claiming that brains are typically male or female because brain structure can be used to predict the sex category (female/male) of the brain’s owner. These formulations are challenged by evidence that sex effects on the brain may be opposite under different conditions, that human brains are comprised of mosaics of female-typical and male-typical features, and that sex category explains only a small part of the variability in human brain structure. These findings led to a new, non-binary, framework, according to which mosaic brains reside in a multi-dimensional space that cannot meaningfully be reduced to a male-female continuum or to a binary variable. This framework may also apply to sex-related variables and has implications for research.},
	urldate = {2025-01-22},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Joel, Daphna},
	month = mar,
	year = {2021},
	keywords = {Brain structure, Human, Hypothalamus, Magnetic resonance imaging (MRI), Male-female continuum, Mosaic, Rat, Stress, Typical female brain, Typical male brain},
	pages = {165--175},
	file = {ScienceDirect Snapshot:/Users/fcalado/Zotero/storage/ICKG869N/S0149763420306540.html:text/html},
}

@article{caliskan:2017,
	title = {Semantics derived automatically from language corpora contain human-like biases},
	volume = {356},
	issn = {0036-8075, 1095-9203},
	url = {http://arxiv.org/abs/1608.07187},
	doi = {10.1126/science.aal4230},
	abstract = {Artificial intelligence and machine learning are in a period of astounding growth. However, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. Here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language---the same sort of language humans are exposed to every day. We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well-known psychological studies. We replicate these using a widely used, purely statistical machine-learning model---namely, the GloVe word embedding---trained on a corpus of text from the Web. Our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the \{{\textbackslash}em status quo\} for the distribution of gender with respect to careers or first names. These regularities are captured by machine learning along with the rest of semantics. In addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results have implications not only for AI and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.},
	number = {6334},
	urldate = {2025-01-22},
	journal = {Science},
	author = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
	month = apr,
	year = {2017},
	note = {arXiv:1608.07187 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
	pages = {183--186},
	file = {Preprint PDF:/Users/fcalado/Zotero/storage/9DHS2NRB/Caliskan et al. - 2017 - Semantics derived automatically from language corp.pdf:application/pdf;Snapshot:/Users/fcalado/Zotero/storage/PHUZLBKE/1608.html:text/html},
}

@misc{bolukbasi:2016,
	title = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}? {Debiasing} {Word} {Embeddings}},
	shorttitle = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}?},
	url = {http://arxiv.org/abs/1607.06520},
	doi = {10.48550/arXiv.1607.06520},
	abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to "debias" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
	month = jul,
	year = {2016},
	note = {arXiv:1607.06520 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/fcalado/Zotero/storage/RSIXBD89/Bolukbasi et al. - 2016 - Man is to Computer Programmer as Woman is to Homem.pdf:application/pdf;Snapshot:/Users/fcalado/Zotero/storage/RCCD2XMR/1607.html:text/html},
}

@misc{zhao:2018,
	title = {Gender {Bias} in {Coreference} {Resolution}: {Evaluation} and {Debiasing} {Methods}},
	shorttitle = {Gender {Bias} in {Coreference} {Resolution}},
	url = {http://arxiv.org/abs/1804.06876},
	doi = {10.48550/arXiv.1804.06876},
	abstract = {We introduce a new benchmark, WinoBias, for coreference resolution focused on gender bias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing coreference benchmark datasets. Our dataset and code are available at http://winobias.org.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
	month = apr,
	year = {2018},
	note = {arXiv:1804.06876 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/fcalado/Zotero/storage/JT4HYD9H/Zhao et al. - 2018 - Gender Bias in Coreference Resolution Evaluation .pdf:application/pdf;Snapshot:/Users/fcalado/Zotero/storage/EZCWKVJ2/1804.html:text/html},
}

@misc{meade:2022,
	title = {An {Empirical} {Survey} of the {Effectiveness} of {Debiasing} {Techniques} for {Pre}-trained {Language} {Models}},
	url = {http://arxiv.org/abs/2110.08527},
	doi = {10.48550/arXiv.2110.08527},
	abstract = {Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model's language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Meade, Nicholas and Poole-Dayan, Elinor and Reddy, Siva},
	month = apr,
	year = {2022},
	note = {arXiv:2110.08527 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/fcalado/Zotero/storage/LQ78DMEV/Meade et al. - 2022 - An Empirical Survey of the Effectiveness of Debias.pdf:application/pdf;Snapshot:/Users/fcalado/Zotero/storage/BSDR6EEG/2110.html:text/html},
}

@misc{nemani:2023,
	title = {Gender {Bias} in {Transformer} {Models}: {A} comprehensive survey},
	shorttitle = {Gender {Bias} in {Transformer} {Models}},
	url = {http://arxiv.org/abs/2306.10530},
	doi = {10.48550/arXiv.2306.10530},
	abstract = {Gender bias in artificial intelligence (AI) has emerged as a pressing concern with profound implications for individuals' lives. This paper presents a comprehensive survey that explores gender bias in Transformer models from a linguistic perspective. While the existence of gender bias in language models has been acknowledged in previous studies, there remains a lack of consensus on how to effectively measure and evaluate this bias. Our survey critically examines the existing literature on gender bias in Transformers, shedding light on the diverse methodologies and metrics employed to assess bias. Several limitations in current approaches to measuring gender bias in Transformers are identified, encompassing the utilization of incomplete or flawed metrics, inadequate dataset sizes, and a dearth of standardization in evaluation methods. Furthermore, our survey delves into the potential ramifications of gender bias in Transformers for downstream applications, including dialogue systems and machine translation. We underscore the importance of fostering equity and fairness in these systems by emphasizing the need for heightened awareness and accountability in developing and deploying language technologies. This paper serves as a comprehensive overview of gender bias in Transformer models, providing novel insights and offering valuable directions for future research in this critical domain.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Nemani, Praneeth and Joel, Yericherla Deepak and Vijay, Palla and Liza, Farhana Ferdousi},
	month = jun,
	year = {2023},
	note = {arXiv:2306.10530 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/fcalado/Zotero/storage/NTXBE6AH/Nemani et al. - 2023 - Gender Bias in Transformer Models A comprehensive.pdf:application/pdf;Snapshot:/Users/fcalado/Zotero/storage/EHXL8FPQ/2306.html:text/html},
}

@misc{gonen:2019,
	title = {Lipstick on a {Pig}: {Debiasing} {Methods} {Cover} up {Systematic} {Gender} {Biases} in {Word} {Embeddings} {But} do not {Remove} {Them}},
	shorttitle = {Lipstick on a {Pig}},
	url = {http://arxiv.org/abs/1903.03862},
	doi = {10.48550/arXiv.1903.03862},
	abstract = {Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between "gender-neutralized" words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Gonen, Hila and Goldberg, Yoav},
	month = sep,
	year = {2019},
	note = {arXiv:1903.03862 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/fcalado/Zotero/storage/T2UYG8SA/Gonen and Goldberg - 2019 - Lipstick on a Pig Debiasing Methods Cover up Syst.pdf:application/pdf;Snapshot:/Users/fcalado/Zotero/storage/35DLTRQL/1903.html:text/html},
}

@misc{blodgett:2020,
	title = {Language ({Technology}) is {Power}: {A} {Critical} {Survey} of "{Bias}" in {NLP}},
	shorttitle = {Language ({Technology}) is {Power}},
	url = {http://arxiv.org/abs/2005.14050},
	doi = {10.48550/arXiv.2005.14050},
	abstract = {We survey 146 papers analyzing "bias" in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing "bias" is an inherently normative process. We further find that these papers' proposed quantitative techniques for measuring or mitigating "bias" are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing "bias" in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of "bias"---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements---and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Blodgett, Su Lin and Barocas, Solon and III, Hal Daumé and Wallach, Hanna},
	month = may,
	year = {2020},
	note = {arXiv:2005.14050 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {Preprint PDF:/Users/fcalado/Zotero/storage/PJ99TSNP/Blodgett et al. - 2020 - Language (Technology) is Power A Critical Survey .pdf:application/pdf;Snapshot:/Users/fcalado/Zotero/storage/NTE4GBKU/2005.html:text/html},
}

@inproceedings{blodgett:2021,
	address = {Online},
	title = {Stereotyping {Norwegian} {Salmon}: {An} {Inventory} of {Pitfalls} in {Fairness} {Benchmark} {Datasets}},
	shorttitle = {Stereotyping {Norwegian} {Salmon}},
	url = {https://aclanthology.org/2021.acl-long.81},
	doi = {10.18653/v1/2021.acl-long.81},
	abstract = {Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system’s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens—originating from the social sciences—to inventory a range of pitfalls that threaten these benchmarks’ validity as measurement models for stereotyping. We ﬁnd that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.},
	language = {en},
	urldate = {2025-01-22},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Blodgett, Su Lin and Lopez, Gilsinia and Olteanu, Alexandra and Sim, Robert and Wallach, Hanna},
	year = {2021},
	pages = {1004--1015},
	file = {Blodgett et al. - 2021 - Stereotyping Norwegian Salmon An Inventory of Pit.pdf:/Users/fcalado/Zotero/storage/KNU349ZU/Blodgett et al. - 2021 - Stereotyping Norwegian Salmon An Inventory of Pit.pdf:application/pdf},
}

@inproceedings{devinney:2022,
	address = {New York, NY, USA},
	series = {{FAccT} '22},
	title = {Theories of “{Gender}” in {NLP} {Bias} {Research}},
	isbn = {978-1-4503-9352-2},
	url = {https://dl.acm.org/doi/10.1145/3531146.3534627},
	doi = {10.1145/3531146.3534627},
	abstract = {The rise of concern around Natural Language Processing (NLP) technologies containing and perpetuating social biases has led to a rich and rapidly growing area of research. Gender bias is one of the central biases being analyzed, but to date there is no comprehensive analysis of how “gender” is theorized in the field. We survey nearly 200 articles concerning gender bias in NLP to discover how the field conceptualizes gender both explicitly (e.g. through definitions of terms) and implicitly (e.g. through how gender is operationalized in practice). In order to get a better idea of emerging trajectories of thought, we split these articles into two sections by time. We find that the majority of the articles do not make their theorization of gender explicit, even if they clearly define “bias.” Almost none use a model of gender that is intersectional or inclusive of nonbinary genders; and many conflate sex characteristics, social gender, and linguistic gender in ways that disregard the existence and experience of trans, nonbinary, and intersex people. There is an increase between the two time-sections in statements acknowledging that gender is a complicated reality, however, very few articles manage to put this acknowledgment into practice. In addition to analyzing these findings, we provide specific recommendations to facilitate interdisciplinary work, and to incorporate theory and methodology from Gender Studies. Our hope is that this will produce more inclusive gender bias research in NLP.},
	urldate = {2025-01-22},
	booktitle = {Proceedings of the 2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Devinney, Hannah and Björklund, Jenny and Björklund, Henrik},
	month = jun,
	year = {2022},
	pages = {2083--2102},
	file = {Full Text PDF:/Users/fcalado/Zotero/storage/5LGRZS7C/Devinney et al. - 2022 - Theories of “Gender” in NLP Bias Research.pdf:application/pdf},
}

@inproceedings{caliskan:2022,
	title = {Gender {Bias} in {Word} {Embeddings}: {A} {Comprehensive} {Analysis} of {Frequency}, {Syntax}, and {Semantics}},
	shorttitle = {Gender {Bias} in {Word} {Embeddings}},
	url = {http://arxiv.org/abs/2206.03390},
	doi = {10.1145/3514094.3534162},
	abstract = {The statistical regularities in language corpora encode well-known social biases into word embeddings. Here, we focus on gender to provide a comprehensive analysis of group-based biases in widely-used static English word embeddings trained on internet corpora (GloVe 2014, fastText 2017). Using the Single-Category Word Embedding Association Test, we demonstrate the widespread prevalence of gender biases that also show differences in: (1) frequencies of words associated with men versus women; (b) part-of-speech tags in gender-associated words; (c) semantic categories in gender-associated words; and (d) valence, arousal, and dominance in gender-associated words. First, in terms of word frequency: we find that, of the 1,000 most frequent words in the vocabulary, 77\% are more associated with men than women, providing direct evidence of a masculine default in the everyday language of the English-speaking world. Second, turning to parts-of-speech: the top male-associated words are typically verbs (e.g., fight, overpower) while the top female-associated words are typically adjectives and adverbs (e.g., giving, emotionally). Gender biases in embeddings also permeate parts-of-speech. Third, for semantic categories: bottom-up, cluster analyses of the top 1,000 words associated with each gender. The top male-associated concepts include roles and domains of big tech, engineering, religion, sports, and violence; in contrast, the top female-associated concepts are less focused on roles, including, instead, female-specific slurs and sexual content, as well as appearance and kitchen terms. Fourth, using human ratings of word valence, arousal, and dominance from a {\textasciitilde}20,000 word lexicon, we find that male-associated words are higher on arousal and dominance, while female-associated words are higher on valence.},
	urldate = {2025-01-22},
	booktitle = {Proceedings of the 2022 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	author = {Caliskan, Aylin and Ajay, Pimparkar Parth and Charlesworth, Tessa and Wolfe, Robert and Banaji, Mahzarin R.},
	month = jul,
	year = {2022},
	note = {arXiv:2206.03390 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
	pages = {156--170},
	file = {Preprint PDF:/Users/fcalado/Zotero/storage/ZHKV8W9E/Caliskan et al. - 2022 - Gender Bias in Word Embeddings A Comprehensive An.pdf:application/pdf;Snapshot:/Users/fcalado/Zotero/storage/79HH4KHD/2206.html:text/html},
}

@inproceedings{hitti:2019,
	address = {Florence, Italy},
	title = {Proposed {Taxonomy} for {Gender} {Bias} in {Text}; {A} {Filtering} {Methodology} for the {Gender} {Generalization} {Subtype}},
	url = {https://aclanthology.org/W19-3802/},
	doi = {10.18653/v1/W19-3802},
	abstract = {The purpose of this paper is to present an empirical study on gender bias in text. Current research in this field is focused on detecting and correcting for gender bias in existing machine learning models rather than approaching the issue at the dataset level. The underlying motivation is to create a dataset which could enable machines to learn to differentiate bias writing from non-bias writing. A taxonomy is proposed for structural and contextual gender biases which can manifest themselves in text. A methodology is proposed to fetch one type of structural gender bias, Gender Generalization. We explore the IMDB movie review dataset and 9 different corpora from Project Gutenberg. By filtering out irrelevant sentences, the remaining pool of candidate sentences are sent for human validation. A total of 6123 judgments are made on 1627 sentences and after a quality check on randomly selected sentences we obtain an accuracy of 75\%. Out of the 1627 sentences, 808 sentence were labeled as Gender Generalizations. The inter-rater reliability amongst labelers was of 61.14\%.},
	urldate = {2025-01-22},
	booktitle = {Proceedings of the {First} {Workshop} on {Gender} {Bias} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Hitti, Yasmeen and Jang, Eunbee and Moreno, Ines and Pelletier, Carolyne},
	editor = {Costa-jussà, Marta R. and Hardmeier, Christian and Radford, Will and Webster, Kellie},
	month = aug,
	year = {2019},
	pages = {8--17},
	file = {Full Text PDF:/Users/fcalado/Zotero/storage/F4QDNE6Y/Hitti et al. - 2019 - Proposed Taxonomy for Gender Bias in Text\; A Filte.pdf:application/pdf},
}

@inproceedings{klein:2024-1,
	address = {Rio de Janeiro Brazil},
	title = {Data {Feminism} for {AI}},
	isbn = {9798400704505},
	url = {https://dl.acm.org/doi/10.1145/3630106.3658543},
	doi = {10.1145/3630106.3658543},
	abstract = {This paper presents a set of intersectional feminist principles for conducting equitable, ethical, and sustainable AI research. In Data Feminism (2020), we offered seven principles for examining and challenging unequal power in data science. Here, we present a rationale for why feminism remains deeply relevant for AI research, rearticulate the original principles of data feminism with respect to AI, and introduce two potential new principles related to environmental impact and consent. Together, these principles help to 1) account for the unequal, undemocratic, extractive, and exclusionary forces at work in AI research, development, and deployment; 2) identify and mitigate predictable harms in advance of unsafe, discriminatory, or otherwise oppressive systems being released into the world; and 3) inspire creative, joyful, and collective ways to work towards a more equitable, sustainable world in which all of us can thrive.},
	language = {en},
	urldate = {2025-01-22},
	booktitle = {The 2024 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Klein, Lauren and D'Ignazio, Catherine},
	month = jun,
	year = {2024},
	pages = {100--112},
	file = {Klein and D'Ignazio - 2024 - Data Feminism for AI.pdf:/Users/fcalado/Zotero/storage/NMT9E3DY/Klein and D'Ignazio - 2024 - Data Feminism for AI.pdf:application/pdf},
}

@inproceedings{birhane:2022,
	address = {Seoul Republic of Korea},
	title = {The {Values} {Encoded} in {Machine} {Learning} {Research}},
	isbn = {978-1-4503-9352-2},
	url = {https://dl.acm.org/doi/10.1145/3531146.3533083},
	doi = {10.1145/3531146.3533083},
	abstract = {Machine learning currently exerts an outsized influence on the world, increasingly affecting institutional practices and impacted communities. It is therefore critical that we question vague conceptions of the field as value-neutral or universally beneficial, and investigate what specific values the field is advancing. In this paper, we first introduce a method and annotation scheme for studying the values encoded in documents such as research papers. Applying the scheme, we analyze 100 highly cited machine learning papers published at premier machine learning conferences, ICML and NeurIPS. We annotate key features of papers which reveal their values: their justification for their choice of project, which attributes of their project they uplift, their consideration of potential negative consequences, and their institutional affiliations and funding sources. We find that few of the papers justify how their project connects to a societal need (15\%) and far fewer discuss negative potential (1\%). Through line-by-line content analysis, we identify 59 values that are uplifted in ML research, and, of these, we find that the papers most frequently justify and assess themselves based on Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty. We present extensive textual evidence and identify key themes in the definitions and operationalization of these values. Notably, we find systematic textual evidence that these top values are being defined and applied with assumptions and implications generally supporting the centralization of power. Finally, we find increasingly close ties between these highly cited papers and tech companies and elite universities.},
	language = {en},
	urldate = {2025-01-22},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Birhane, Abeba and Kalluri, Pratyusha and Card, Dallas and Agnew, William and Dotan, Ravit and Bao, Michelle},
	month = jun,
	year = {2022},
	pages = {173--184},
	file = {Birhane et al. - 2022 - The Values Encoded in Machine Learning Research.pdf:/Users/fcalado/Zotero/storage/NLPZ874K/Birhane et al. - 2022 - The Values Encoded in Machine Learning Research.pdf:application/pdf},
}

@inproceedings{barocas:2017,
	title = {The problem with bias: from allocative  to representational harms in machine learning.},
	author = {Barocas, Solon and Crawford, Kate and Shapiro, Aaron and Wallach, Hanna},
	year = {2017},
}

@book{butler:1993,
	title = {Bodies that {Matter}: {On} the {Discursive} {Limits} of "sex"},
	isbn = {978-0-415-90366-0},
	shorttitle = {Bodies that {Matter}},
	abstract = {In Bodies That Matter, Judith Butler further develops her distinctive theory of gender by examining the workings of power at the most "material" dimensions of sex and sexuality. Deepening the inquiries she began in Gender Trouble, Butler offers an original reformulation of the materiality of bodies, examining how the power of heterosexual hegemony forms the "matter" of bodies, sex, and gender. Butler argues that power operates to constrain "sex" from the start, delimiting what counts as a viable sex. She offers a clarification of the notion of "performativity" introduced in Gender Trouble and explores the meaning of a citational politics. The text includes readings of Plato, Irigaray, Lacan, and Freud on the formation of materiality and bodily boundaries; "Paris is Burning," Nella Larsen's "Passing," and short stories by Willa Cather; along with a reconsideration of "performativity" and politics in feminist, queer, and radical democratic theory.},
	language = {en},
	publisher = {Psychology Press},
	author = {Butler, Judith},
	year = {1993},
	note = {Google-Books-ID: ZqiIgwQiyFYC},
	keywords = {Social Science / Feminism \& Feminist Theory, Social Science / Gender Studies, Philosophy / General, Psychology / General, Psychology / Human Sexuality, Social Science / Women's Studies},
}

@book{munoz:2009,
	title = {Cruising {Utopia}: {The} {Then} and {There} of {Queer} {Futurity}},
	isbn = {978-0-8147-5728-4},
	shorttitle = {Cruising {Utopia}},
	abstract = {The LGBT agenda for too long has been dominated by pragmatic issues like same-sex marriage and gays in the military. It has been stifled by this myopic focus on the present, which is short-sighted and assimilationist. Cruising Utopia seeks to break the present stagnancy by cruising ahead. Drawing on the work of Ernst Bloch, José Esteban Muñoz recalls the queer past for guidance in presaging its future. He considers the work of seminal artists and writers such as Andy Warhol, LeRoi Jones, Frank O’Hara, Ray Johnson, Fred Herko, Samuel Delany, and Elizabeth Bishop, alongside contemporary performance and visual artists like Dynasty Handbag, My Barbarian, Luke Dowd, Tony Just, and Kevin McCarty in order to decipher the anticipatory illumination of art and its uncanny ability to open windows to the future. In a startling repudiation of what the LGBT movement has held dear, Muñoz contends that queerness is instead a futurity bound phenomenon, a \&quot;not yet here\&quot; that critically engages pragmatic presentism. Part manifesto, part love-letter to the past and the future, Cruising Utopia argues that the here and now are not enough and issues an urgent call for the revivification of the queer political imagination.},
	language = {en},
	publisher = {NYU Press},
	author = {Muñoz, José Esteban},
	month = nov,
	year = {2009},
	note = {Google-Books-ID: f1MTCgAAQBAJ},
	keywords = {Art / Criticism \& Theory, Social Science / Gay Studies, Social Science / Lesbian Studies},
}

@book{butler:1990,
	title = {Gender {Trouble}: {Feminism} and the {Subversion} of {Identity}},
	isbn = {978-1-136-78324-1},
	shorttitle = {Gender {Trouble}},
	abstract = {One of the most talked-about scholarly works of the past fifty years, Judith Butler’s Gender Trouble is as celebrated as it is controversial. Arguing that traditional feminism is wrong to look to a natural, 'essential' notion of the female, or indeed of sex or gender, Butler starts by questioning the category 'woman' and continues in this vein with examinations of 'the masculine' and 'the feminine'. Best known however, but also most often misinterpreted, is Butler's concept of gender as a reiterated social performance rather than the expression of a prior reality. Thrilling and provocative, few other academic works have roused passions to the same extent.},
	language = {en},
	publisher = {Routledge},
	author = {Butler, Judith},
	year = {1990},
	note = {Google-Books-ID: gTbbCgAAQBAJ},
	keywords = {Social Science / Feminism \& Feminist Theory, Social Science / Gender Studies, Philosophy / General, Psychology / General, Psychology / Human Sexuality, Literary Criticism / Semiotics \& Theory, Literary Criticism / Women Authors, Social Science / Anthropology / Cultural \& Social, Social Science / Media Studies},
}

@book{sedgwick:1990,
	title = {Epistemology of the {Closet}, {Updated} with a {New} {Preface}},
	isbn = {978-0-520-93448-1},
	abstract = {Since the late 1980s, queer studies and theory have become vital to the intellectual and political life of the United States. This has been due, in no small degree, to the influence of Eve Kosofsky Sedgwick's critically acclaimed Epistemology of the Closet. Working from classic texts of European and American writers—including Melville, James, Nietzsche, Proust, and Wilde—Sedgwick analyzes a turn-of-the-century historical moment in which sexual orientation became as important a demarcation of personhood as gender had been for centuries. In her preface to this updated edition Sedgwick places the book both personally and historically, looking specifically at the horror of the first wave of the AIDS epidemic and its influence on the text.},
	language = {en},
	publisher = {University of California Press},
	author = {Sedgwick, Eve Kosofsky},
	year = {1990},
	note = {Google-Books-ID: KMhUa25EPkIC},
	keywords = {Social Science / Gender Studies, Literary Criticism / Semiotics \& Theory},
}

@book{hooks:2000,
	title = {Feminist {Theory}: {From} {Margin} to {Center}},
	isbn = {978-0-7453-1663-5},
	shorttitle = {Feminist {Theory}},
	abstract = {'[An] intelligently critical, inclusive, personal and very accessible feminist polemic... A very good book, promoting a feminism which includes women of all classes and colours, and which does not exclude men either. The experience of ordinary people, rather than largely white female academics, is at its heart. Still very relevant, and recommended.' Theory.orgFeminist Theory established bell hooks as one of international feminism's most challenging and influential voices. This edition includes a new preface by the author, reflecting on the book's impact and the development of her ideas since it was first published. In this beautifully written and carefully argued work, hooks maintains that mainstream feminism's reliance on white, middle-class, and professional spokeswomen obscures the involvement, leadership, and centrality of women of colour and poor women in the movement for women's liberation. Hooks argues that feminism's goal of seeking credibility and acceptance on already existing ground - rather than demanding the lasting and more fundamental transformation of society - has shortchanged the movement.A sweeping examination of the core issues of sexual politics, Feminist Theory argues that contemporary feminists must acknowledge the full complexity and diversity of women's experience to create a mass movement to end women's oppression.},
	language = {en},
	publisher = {Pluto Press},
	author = {hooks, bell},
	year = {2000},
	note = {Google-Books-ID: uvIQbop4cdsC},
	keywords = {Social Science / Feminism \& Feminist Theory, Social Science / Sociology / General, Political Science / Political Ideologies / Democracy},
}

@misc{allenai:2021,
	address = {https://huggingface.co/datasets/allenai/c4},
	title = {C4},
	author = {{AllenAI}},
	year = {2021},
}

@misc{raffel:2023,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	doi = {10.48550/arXiv.1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = sep,
	year = {2023},
	note = {arXiv:1910.10683 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/fcalado/Zotero/storage/DDJ6EEYC/Raffel et al. - 2023 - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf;Snapshot:/Users/fcalado/Zotero/storage/NJNGV5XA/1910.html:text/html},
}

@misc{thoppilan:2022,
	title = {{LaMDA}: {Language} {Models} for {Dialog} {Applications}},
	shorttitle = {{LaMDA}},
	url = {http://arxiv.org/abs/2201.08239},
	doi = {10.48550/arXiv.2201.08239},
	abstract = {We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Thoppilan, Romal and Freitas, Daniel De and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and Meier-Hellstern, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and Hoffman-John, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
	month = feb,
	year = {2022},
	note = {arXiv:2201.08239 [cs]
version: 3},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/fcalado/Zotero/storage/U7ZP24SK/Thoppilan et al. - 2022 - LaMDA Language Models for Dialog Applications.pdf:application/pdf;Snapshot:/Users/fcalado/Zotero/storage/2X5QJMLH/2201.html:text/html},
}

@inproceedings{bender:2021,
	address = {Virtual Event Canada},
	title = {On the {Dangers} of {Stochastic} {Parrots}: {Can} {Language} {Models} {Be} {Too} {Big}? 🦜},
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the {Dangers} of {Stochastic} {Parrots}},
	url = {https://dl.acm.org/doi/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	language = {en},
	urldate = {2025-01-22},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	month = mar,
	year = {2021},
	pages = {610--623},
	file = {Bender et al. - 2021 - On the Dangers of Stochastic Parrots Can Language.pdf:/Users/fcalado/Zotero/storage/J74XWRE6/Bender et al. - 2021 - On the Dangers of Stochastic Parrots Can Language.pdf:application/pdf},
}

@misc{ldnoobw:2012,
	title = {{LDNOOBW}/{List}-of-{Dirty}-{Naughty}-{Obscene}-and-{Otherwise}-{Bad}-{Words}},
	copyright = {CC-BY-4.0},
	url = {https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words},
	abstract = {List of Dirty, Naughty, Obscene, and Otherwise Bad Words},
	urldate = {2025-01-22},
	publisher = {LDNOOBW},
	author = {{LDNOOBW}},
	year = {2012},
	note = {original-date: 2012-03-09T02:15:36Z},
}

@article{krantz:1995,
	title = {Reconsidering the {Etymology} of {Bulldike}},
	volume = {70},
	url = {https://www.jstor.org/stable/455819},
	number = {2},
	urldate = {2025-01-22},
	journal = {American Speech},
	author = {Krantz, Susan E.},
	year = {1995},
	pages = {217--221},
	file = {Reconsidering the Etymology of Bulldike on JSTOR:/Users/fcalado/Zotero/storage/NDG59IXA/455819.html:text/html},
}

@inproceedings{dodge:2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Documenting {Large} {Webtext} {Corpora}: {A} {Case} {Study} on the {Colossal} {Clean} {Crawled} {Corpus}},
	shorttitle = {Documenting {Large} {Webtext} {Corpora}},
	url = {https://aclanthology.org/2021.emnlp-main.98/},
	doi = {10.18653/v1/2021.emnlp-main.98},
	abstract = {Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.},
	urldate = {2025-01-22},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Dodge, Jesse and Sap, Maarten and Marasović, Ana and Agnew, William and Ilharco, Gabriel and Groeneveld, Dirk and Mitchell, Margaret and Gardner, Matt},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {1286--1305},
	file = {Full Text PDF:/Users/fcalado/Zotero/storage/4KPK6PBJ/Dodge et al. - 2021 - Documenting Large Webtext Corpora A Case Study on.pdf:application/pdf},
}

@article{greenwald:1998,
	title = {Measuring individual differences in implicit cognition: {The} implicit association test},
	volume = {74},
	issn = {1939-1315},
	shorttitle = {Measuring individual differences in implicit cognition},
	doi = {10.1037/0022-3514.74.6.1464},
	abstract = {An implicit association test (IAT) measures differential association of 2 target concepts with an attribute. The 2 concepts appear in a 2-choice task (e.g., flower vs. insect names), and the attribute in a 2nd task (e.g., pleasant vs. unpleasant words for an evaluation attribute). When instructions oblige highly associated categories (e.g., flower + pleasant) to share a response key, performance is faster than when less associated categories (e.g., insect + pleasant) share a key. This performance difference implicitly measures differential association of the 2 concepts with the attribute. In 3 experiments, the IAT was sensitive to (a) near-universal evaluative differences (e.g., flower vs. insect), (b) expected individual differences in evaluative associations (Japanese + pleasant vs. Korean + pleasant for Japanese vs. Korean subjects), and (c) consciously disavowed evaluative differences (Black + pleasant vs. White + pleasant for self-described unprejudiced White subjects). (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {6},
	journal = {Journal of Personality and Social Psychology},
	author = {Greenwald, Anthony G. and McGhee, Debbie E. and Schwartz, Jordan L. K.},
	year = {1998},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Associative Processes, Attitude Measures, Individual Differences, Judgment, Test Construction},
	pages = {1464--1480},
	file = {Full Text:/Users/fcalado/Zotero/storage/JFSVRXJX/Greenwald et al. - 1998 - Measuring individual differences in implicit cogni.pdf:application/pdf;Snapshot:/Users/fcalado/Zotero/storage/8LG4U3KT/doiLanding.html:text/html},
}

@misc{greenwald:2011,
	title = {About the {IAT}},
	url = {https://implicit.harvard.edu/implicit/iatdetails.html},
	urldate = {2025-01-22},
	author = {Greenwald, Tony and Banaji, Mahzarin and Nosek, Brian and Teachman, Bethany and Nock, Matt},
	year = {2011},
	file = {About the IAT:/Users/fcalado/Zotero/storage/2QUS3XUY/iatdetails.html:text/html},
}

@misc{schimmack,
	title = {The {Implicit} {Association} {Test}: {A} {Method} in {Search} of a {Construct} - {Ulrich} {Schimmack}, 2021},
	url = {https://journals.sagepub.com/doi/10.1177/1745691619863798},
	urldate = {2025-01-22},
	author = {Schimmack, Ulrich},
	file = {The Implicit Association Test\: A Method in Search of a Construct - Ulrich Schimmack, 2021:/Users/fcalado/Zotero/storage/ZRA2CLJT/1745691619863798.html:text/html},
}

@article{karpinski:2001,
	title = {Attitudes and the {Implicit} {Association} {Test}},
	volume = {81},
	issn = {1939-1315},
	doi = {10.1037/0022-3514.81.5.774},
	abstract = {Three studies examined the relationship between the Implicit Association Test (IAT; A. G. Greenwald, D. E. McGhee, \& J. L. K. Schwartz, 1998) and explicit attitudes. In the 1st and all subsequent studies, the lack of any correlation between the IAT and explicitly measured attitudes supports the view that the IAT is independent from explicit attitudes. Study 2 examined the relationships among the IAT, explicit attitudes, and behavior and found that the explicit attitudes predicted behavior but the IAT did not. Finally, in Study 3 it was found that the IAT was affected by exposing participants to new associations between attitude objects, whereas the explicit attitudes remained unchanged. Taken together, these results support an environmental association interpretation of the IAT in which IAT scores reflect the associations a person has been exposed to in his or her environment rather than the extent to which the person endorses those evaluative associations. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
	number = {5},
	journal = {Journal of Personality and Social Psychology},
	author = {Karpinski, Andrew and Hilton, James L.},
	year = {2001},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Associative Processes, Attitude Measures, Adult Attitudes, Behavior},
	pages = {774--788},
	file = {Snapshot:/Users/fcalado/Zotero/storage/FF3QTTMH/doiLanding.html:text/html},
}

@inproceedings{wolfe:2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Low {Frequency} {Names} {Exhibit} {Bias} and {Overfitting} in {Contextualizing} {Language} {Models}},
	url = {https://aclanthology.org/2021.emnlp-main.41/},
	doi = {10.18653/v1/2021.emnlp-main.41},
	abstract = {We use a dataset of U.S. first names with labels based on predominant gender and racial group to examine the effect of training corpus frequency on tokenization, contextualization, similarity to initial representation, and bias in BERT, GPT-2, T5, and XLNet. We show that predominantly female and non-white names are less frequent in the training corpora of these four language models. We find that infrequent names are more self-similar across contexts, with Spearman`s rho between frequency and self-similarity as low as -.763. Infrequent names are also less similar to initial representation, with Spearman`s rho between frequency and linear centered kernel alignment (CKA) similarity to initial representation as high as .702. Moreover, we find Spearman`s rho between racial bias and name frequency in BERT of .492, indicating that lower-frequency minority group names are more associated with unpleasantness. Representations of infrequent names undergo more processing, but are more self-similar, indicating that models rely on less context-informed representations of uncommon and minority names which are overfit to a lower number of observed contexts.},
	urldate = {2025-01-23},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wolfe, Robert and Caliskan, Aylin},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {518--532},
	file = {Full Text PDF:/Users/fcalado/Zotero/storage/JK2F9LVR/Wolfe and Caliskan - 2021 - Low Frequency Names Exhibit Bias and Overfitting i.pdf:application/pdf},
}

@article{vanloon:2022,
	title = {Negative {Associations} in {Word} {Embeddings} {Predict} {Anti}-black {Bias} across {Regions}–but {Only} via {Name} {Frequency}},
	volume = {16},
	issn = {2162-3449},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10147343/},
	doi = {10.1609/icwsm.v16i1.19399},
	abstract = {The word embedding association test (WEAT) is an important method for measuring linguistic biases against social groups such as ethnic minorities in large text corpora. It does so by comparing the semantic relatedness of words prototypical of the groups (e.g., names unique to those groups) and attribute words (e.g., ‘pleasant’ and ‘unpleasant’ words). We show that anti-Black WEAT estimates from geo-tagged social media data at the level of metropolitan statistical areas strongly correlate with several measures of racial animus—even when controlling for sociodemographic covariates. However, we also show that every one of these correlations is explained by a third variable: the frequency of Black names in the underlying corpora relative to White names. This occurs because word embeddings tend to group positive (negative) words and frequent (rare) words together in the estimated semantic space. As the frequency of Black names on social media is strongly correlated with Black Americans’ prevalence in the population, this results in spuriously high anti-Black WEAT estimates wherever few Black Americans live. This suggests that research using the WEAT to measure bias should consider term frequency, and also demonstrates the potential consequences of using black-box models like word embeddings to study human cognition and behavior.},
	urldate = {2025-01-23},
	journal = {Proceedings of the ... International AAAI Conference on Weblogs and Social Media. International AAAI Conference on Weblogs and Social Media},
	author = {van Loon, Austin and Giorgi, Salvatore and Willer, Robb and Eichstaedt, Johannes},
	month = may,
	year = {2022},
	pmid = {37122435},
	pmcid = {PMC10147343},
	pages = {1419--1424},
	file = {PubMed Central Full Text PDF:/Users/fcalado/Zotero/storage/PBUZDHBX/van Loon et al. - 2022 - Negative Associations in Word Embeddings Predict A.pdf:application/pdf},
}

@book{love:2009,
	title = {Feeling {Backward}: {Loss} and the {Politics} of {Queer} {History}},
	isbn = {978-0-674-03239-2},
	shorttitle = {Feeling {Backward}},
	abstract = {Feeling Backward weighs the costs of the contemporary move to the mainstream in lesbian and gay culture. While the widening tolerance for same-sex marriage and for gay-themed media brings clear benefits, gay assimilation entails other losses--losses that have been hard to identify or mourn, since many aspects of historical gay culture are so closely associated with the pain and shame of the closet.Feeling Backward makes an effort to value aspects of historical gay experience that now threaten to disappear, branded as embarrassing evidence of the bad old days before Stonewall. It looks at early-twentieth-century queer novels often dismissed as "too depressing" and asks how we might value and reclaim the dark feelings that they represent. Heather Love argues that instead of moving on, we need to look backward and consider how this history continues to affect us in the present.Through elegant readings of Walter Pater, Willa Cather, Radclyffe Hall, and Sylvia Townsend Warner, and through stimulating engagement with a range of critical sources, Feeling Backward argues for a form of politics attentive to social exclusion and its effects.},
	language = {en},
	publisher = {Harvard University Press},
	author = {Love, Heather},
	month = mar,
	year = {2009},
	keywords = {Education / General, History / LGBTQ, History / Social History, Literary Criticism / LGBTQ, Social Science / LGBTQ+ Studies / Gay Studies, Social Science / LGBTQ+ Studies / General, Social Science / LGBTQ+ Studies / Lesbian Studies, Social Science / Popular Culture, Social Science / Sociology / General},
}


@inproceedings{may:2019,
	address = {Minneapolis, Minnesota},
	title = {On {Measuring} {Social} {Biases} in {Sentence} {Encoders}},
	url = {http://aclweb.org/anthology/N19-1063},
	doi = {10.18653/v1/N19-1063},
	abstract = {The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-theart methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difﬁcult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test’s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.},
	language = {en},
	urldate = {2025-01-23},
	booktitle = {Proceedings of the 2019 {Conference} of the {North}},
	publisher = {Association for Computational Linguistics},
	author = {May, Chandler and Wang, Alex and Bordia, Shikha and Bowman, Samuel R. and Rudinger, Rachel},
	year = {2019},
	pages = {622--628},
	file = {May et al. - 2019 - On Measuring Social Biases in Sentence Encoders.pdf:/Users/fcalado/Zotero/storage/FDUFATWB/May et al. - 2019 - On Measuring Social Biases in Sentence Encoders.pdf:application/pdf},
}

@inproceedings{liang:2020,
	address = {Online},
	title = {Towards {Debiasing} {Sentence} {Representations}},
	url = {https://aclanthology.org/2020.acl-main.488/},
	doi = {10.18653/v1/2020.acl-main.488},
	abstract = {As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.},
	urldate = {2025-01-23},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Liang, Paul Pu and Li, Irene Mengze and Zheng, Emily and Lim, Yao Chong and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {5502--5515},
	file = {Full Text PDF:/Users/fcalado/Zotero/storage/5SY9NC2H/Liang et al. - 2020 - Towards Debiasing Sentence Representations.pdf:application/pdf},
}

@misc{reif:2023,
	title = {Fighting {Bias} with {Bias}: {Promoting} {Model} {Robustness} by {Amplifying} {Dataset} {Biases}},
	shorttitle = {Fighting {Bias} with {Bias}},
	url = {http://arxiv.org/abs/2305.18917},
	doi = {10.48550/arXiv.2305.18917},
	abstract = {NLP models often rely on superficial cues known as dataset biases to achieve impressive performance, and can fail on examples where these biases do not hold. Recent work sought to develop robust, unbiased models by filtering biased examples from training sets. In this work, we argue that such filtering can obscure the true capabilities of models to overcome biases, which might never be removed in full from the dataset. We suggest that in order to drive the development of models robust to subtle biases, dataset biases should be amplified in the training set. We introduce an evaluation framework defined by a bias-amplified training set and an anti-biased test set, both automatically extracted from existing datasets. Experiments across three notions of bias, four datasets and two models show that our framework is substantially more challenging for models than the original data splits, and even more challenging than hand-crafted challenge sets. Our evaluation framework can use any existing dataset, even those considered obsolete, to test model robustness. We hope our work will guide the development of robust models that do not rely on superficial biases and correlations. To this end, we publicly release our code and data.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Reif, Yuval and Schwartz, Roy},
	month = may,
	year = {2023},
	note = {arXiv:2305.18917 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/fcalado/Zotero/storage/7HLM7LVR/Reif and Schwartz - 2023 - Fighting Bias with Bias Promoting Model Robustnes.pdf:application/pdf;Snapshot:/Users/fcalado/Zotero/storage/3KLUIH9F/2305.html:text/html},
}

@inproceedings{stanovsky:2019,
	address = {Florence, Italy},
	title = {Evaluating {Gender} {Bias} in {Machine} {Translation}},
	url = {https://aclanthology.org/P19-1164/},
	doi = {10.18653/v1/P19-1164},
	abstract = {We present the first challenge set and evaluation protocol for the analysis of gender bias in machine translation (MT). Our approach uses two recent coreference resolution datasets composed of English sentences which cast participants into non-stereotypical gender roles (e.g., “The doctor asked the nurse to help her in the operation”). We devise an automatic gender bias evaluation method for eight target languages with grammatical gender, based on morphological analysis (e.g., the use of female inflection for the word “doctor”). Our analyses show that four popular industrial MT systems and two recent state-of-the-art academic MT models are significantly prone to gender-biased translation errors for all tested target languages. Our data and code are publicly available at https://github.com/gabrielStanovsky/mt\_gender.},
	urldate = {2025-01-23},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Stanovsky, Gabriel and Smith, Noah A. and Zettlemoyer, Luke},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	pages = {1679--1684},
	file = {Full Text PDF:/Users/fcalado/Zotero/storage/NI4IVXK6/Stanovsky et al. - 2019 - Evaluating Gender Bias in Machine Translation.pdf:application/pdf},
}

@inproceedings{hansson:2021,
	address = {Reykjavik, Iceland (Online)},
	title = {The {Swedish} {Winogender} {Dataset}},
	url = {https://aclanthology.org/2021.nodalida-main.52/},
	abstract = {We introduce the SweWinogender test set, a diagnostic dataset to measure gender bias in coreference resolution. It is modelled after the English Winogender benchmark, and is released with reference statistics on the distribution of men and women between occupations and the association between gender and occupation in modern corpus material. The paper discusses the design and creation of the dataset, and presents a small investigation of the supplementary statistics.},
	urldate = {2025-01-23},
	booktitle = {Proceedings of the 23rd {Nordic} {Conference} on {Computational} {Linguistics} ({NoDaLiDa})},
	publisher = {Linköping University Electronic Press, Sweden},
	author = {Hansson, Saga and Mavromatakis, Konstantinos and Adesam, Yvonne and Bouma, Gerlof and Dannélls, Dana},
	editor = {Dobnik, Simon and Øvrelid, Lilja},
	month = may,
	year = {2021},
	pages = {452--459},
	file = {Full Text PDF:/Users/fcalado/Zotero/storage/3LMPAQRW/Hansson et al. - 2021 - The Swedish Winogender Dataset.pdf:application/pdf},
}

@inproceedings{dinan:2020,
	address = {Online},
	title = {Multi-{Dimensional} {Gender} {Bias} {Classification}},
	url = {https://aclanthology.org/2020.emnlp-main.23/},
	doi = {10.18653/v1/2020.emnlp-main.23},
	abstract = {Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a novel, general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a new, crowdsourced evaluation benchmark. Distinguishing between gender bias along multiple dimensions enables us to train better and more fine-grained gender bias classifiers. We show our classifiers are valuable for a variety of applications, like controlling for gender bias in generative models, detecting gender bias in arbitrary text, and classifying text as offensive based on its genderedness.},
	urldate = {2025-01-23},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Dinan, Emily and Fan, Angela and Wu, Ledell and Weston, Jason and Kiela, Douwe and Williams, Adina},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {314--331},
	file = {Full Text PDF:/Users/fcalado/Zotero/storage/UMAD38S4/Dinan et al. - 2020 - Multi-Dimensional Gender Bias Classification.pdf:application/pdf},
}

@misc{felkner:2024,
	title = {{WinoQueer}: {A} {Community}-in-the-{Loop} {Benchmark} for {Anti}-{LGBTQ}+ {Bias} in {Large} {Language} {Models}},
	shorttitle = {{WinoQueer}},
	url = {http://arxiv.org/abs/2306.15087},
	doi = {10.48550/arXiv.2306.15087},
	abstract = {We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community. The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias. Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members. Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities. Note: This version corrects a bug found in evaluation code after publication. General findings have not changed, but tables 5 and 6 and figure 1 have been corrected.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Felkner, Virginia K. and Chang, Ho-Chun Herbert and Jang, Eugene and May, Jonathan},
	month = oct,
	year = {2024},
	note = {arXiv:2306.15087 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {Preprint PDF:/Users/fcalado/Zotero/storage/WQXTL4N7/Felkner et al. - 2024 - WinoQueer A Community-in-the-Loop Benchmark for A.pdf:application/pdf;Snapshot:/Users/fcalado/Zotero/storage/69Z9HGLC/2306.html:text/html},
}

@book{halberstam:1998,
	title = {Female {Masculinity}},
	isbn = {978-1-4780-0127-0},
	url = {https://www.jstor.org/stable/j.ctv11cwb00},
	abstract = {In this quintessential work of queer theory, Jack Halberstam takes aim at the protected status of male masculinity and shows that female masculinity has offered a distinct alternative to it for well over two centuries. Demonstrating how female masculinity is not some bad imitation of virility, but a lively and dramatic staging of hybrid and minority genders, Halberstam catalogs the diversity of gender expressions among masculine women from nineteenth-century pre-lesbian practices to contemporary drag king performances. Through detailed textual readings as well as empirical research, Halberstam uncovers a hidden history of female masculinities while arguing for a more nuanced understanding of gender categories that would incorporate rather than pathologize them. He rereads Anne Lister's diaries and Radclyffe Hall's The Well of Loneliness as foundational assertions of female masculine identity; considers the enigma of the stone butch and the politics surrounding butch/femme roles within lesbian communities; and explores issues of transsexuality among "transgender dykes"-lesbians who pass as men-and female-to-male transsexuals who may find the label of "lesbian" a temporary refuge. Halberstam also tackles such topics as women and boxing, butches in Hollywood and independent cinema, and the phenomenon of male impersonators. Featuring a new preface by the author, this twentieth anniversary edition of Female Masculinity remains as insightful, timely, and necessary as ever.},
	urldate = {2025-01-30},
	publisher = {Duke University Press},
	author = {Halberstam, Jack},
	year = {1998},
	doi = {10.2307/j.ctv11cwb00},
}

@article{amin:2022,
	title = {We are {All} {Nonbinary}: {A} {Brief} {History} of {Accidents}},
	volume = {158},
	issn = {0734-6018},
	shorttitle = {We are {All} {Nonbinary}},
	url = {https://doi.org/10.1525/rep.2022.158.11.106},
	doi = {10.1525/rep.2022.158.11.106},
	number = {1},
	urldate = {2025-03-14},
	journal = {Representations},
	author = {Amin, Kadji},
	month = may,
	year = {2022},
	pages = {106--119},
	file = {Full Text PDF:/Users/fcalado/Zotero/storage/PLYWAGMC/Amin - 2022 - We are All Nonbinary A Brief History of Accidents.pdf:application/pdf;Snapshot:/Users/fcalado/Zotero/storage/U989JHBB/We-are-All-NonbinaryA-Brief-History-of-Accidents.html:text/html},
}

@misc{dong:2024,
	title = {Disclosure and {Mitigation} of {Gender} {Bias} in {LLMs}},
	url = {http://arxiv.org/abs/2402.11190},
	doi = {10.48550/arXiv.2402.11190},
	abstract = {Large Language Models (LLMs) can generate biased responses. Yet previous direct probing techniques contain either gender mentions or predefined gender stereotypes, which are challenging to comprehensively collect. Hence, we propose an indirect probing framework based on conditional generation. This approach aims to induce LLMs to disclose their gender bias even without explicit gender or stereotype mentions. We explore three distinct strategies to disclose explicit and implicit gender bias in LLMs. Our experiments demonstrate that all tested LLMs exhibit explicit and/or implicit gender bias, even when gender stereotypes are not present in the inputs. In addition, an increased model size or model alignment amplifies bias in most cases. Furthermore, we investigate three methods to mitigate bias in LLMs via Hyperparameter Tuning, Instruction Guiding, and Debias Tuning. Remarkably, these methods prove effective even in the absence of explicit genders or stereotypes.},
	language = {en},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Dong, Xiangjue and Wang, Yibo and Yu, Philip S. and Caverlee, James},
	month = feb,
	year = {2024},
	note = {arXiv:2402.11190 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Dong et al. - 2024 - Disclosure and Mitigation of Gender Bias in LLMs.pdf:/Users/fcalado/Zotero/storage/S3879SGA/Dong et al. - 2024 - Disclosure and Mitigation of Gender Bias in LLMs.pdf:application/pdf},
}

@inproceedings{thakur:2023,
	address = {Toronto, Canada},
	title = {Language {Models} {Get} a {Gender} {Makeover}: {Mitigating} {Gender} {Bias} with {Few}-{Shot} {Data} {Interventions}},
	shorttitle = {Language {Models} {Get} a {Gender} {Makeover}},
	url = {https://aclanthology.org/2023.acl-short.30/},
	doi = {10.18653/v1/2023.acl-short.30},
	abstract = {Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.},
	urldate = {2025-04-02},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Thakur, Himanshu and Jain, Atishay and Vaddamanu, Praneetha and Liang, Paul Pu and Morency, Louis-Philippe},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {340--351},
	file = {Full Text PDF:/Users/fcalado/Zotero/storage/GHRNF2V2/Thakur et al. - 2023 - Language Models Get a Gender Makeover Mitigating .pdf:application/pdf},
}

@inproceedings{furniturewala:2024,
	address = {Miami, Florida, USA},
	title = {“{Thinking}” {Fair} and {Slow}: {On} the {Efficacy} of {Structured} {Prompts} for {Debiasing} {Language} {Models}},
	shorttitle = {“{Thinking}” {Fair} and {Slow}},
	url = {https://aclanthology.org/2024.emnlp-main.13/},
	doi = {10.18653/v1/2024.emnlp-main.13},
	abstract = {Existing debiasing techniques are typically training-based or require access to the model`s internals and output distributions, so they are inaccessible to end-users looking to adapt LLM outputs for their particular needs. In this study, we examine whether structured prompting techniques can offer opportunities for fair text generation. We evaluate a comprehensive end-user-focused iterative framework of debiasing that applies System 2 thinking processes for prompts to induce logical, reflective, and critical text generation, with single, multi-step, instruction, and role-based variants. By systematically evaluating many LLMs across many datasets and different prompting strategies, we show that the more complex System 2-based Implicative Prompts significantly improve over other techniques demonstrating lower mean bias in the outputs with competitive performance on the downstream tasks. Our work offers research directions for the design and the potential of end-user-focused evaluative frameworks for LLM use.},
	urldate = {2025-04-02},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Furniturewala, Shaz and Jandial, Surgan and Java, Abhinav and Banerjee, Pragyan and Shahid, Simra and Bhatia, Sumit and Jaidka, Kokil},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {213--227},
	file = {Full Text PDF:/Users/fcalado/Zotero/storage/G6XLJ3XR/Furniturewala et al. - 2024 - “Thinking” Fair and Slow On the Efficacy of Struc.pdf:application/pdf},
}

@misc{kaneko:2024,
	title = {Evaluating {Gender} {Bias} in {Large} {Language} {Models} via {Chain}-of-{Thought} {Prompting}},
	url = {http://arxiv.org/abs/2401.15585},
	doi = {10.48550/arXiv.2401.15585},
	abstract = {There exist both scalable tasks, like reading comprehension and fact-checking, where model performance improves with model size, and unscalable tasks, like arithmetic reasoning and symbolic reasoning, where model performance does not necessarily improve with model size. Large language models (LLMs) equipped with Chain-of-Thought (CoT) prompting are able to make accurate incremental predictions even on unscalable tasks. Unfortunately, despite their exceptional reasoning abilities, LLMs tend to internalize and reproduce discriminatory societal biases. Whether CoT can provide discriminatory or egalitarian rationalizations for the implicit information in unscalable tasks remains an open question.},
	language = {en},
	urldate = {2025-04-02},
	publisher = {arXiv},
	author = {Kaneko, Masahiro and Bollegala, Danushka and Okazaki, Naoaki and Baldwin, Timothy},
	month = jan,
	year = {2024},
	note = {arXiv:2401.15585 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Kaneko et al. - 2024 - Evaluating Gender Bias in Large Language Models vi.pdf:/Users/fcalado/Zotero/storage/UJQ5AW86/Kaneko et al. - 2024 - Evaluating Gender Bias in Large Language Models vi.pdf:application/pdf},
}
